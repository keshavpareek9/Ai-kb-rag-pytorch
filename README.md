ğŸ“š AI Knowledge Base Q&A â€” RAG + PyTorch Reranker + FastAPI + Docker

A lightweight Retrieval-Augmented Generation (RAG) system designed for CPU-only environments.

It provides a FastAPI service that answers natural language questions from your organizationâ€™s knowledge base using:
Embeddings + FAISS for fast retrieval

PyTorch Cross-Encoder Reranker (BERT-base) for improved relevance

FLAN-T5-small (local) for grounded answer generation

Docker container for reproducible, CPU-friendly deployment

ğŸš€ Features

Endpoints:
    GET /health â†’ status check
    
    POST /query â†’ ask a question, get {answer, sources}
    
    POST /train â†’ fine-tune reranker on synthetic or labeled pairs (runs in background)
    
    
Tech choices:

Embeddings: sentence-transformers/all-MiniLM-L6-v2

Vector DB: FAISS (local index)

Reranker: bert-base-uncased fine-tuned on synthetic FAQ pairs

Generator: google/flan-t5-small (fast on CPU)

    Persistence:
    data/ â†’ raw KB + training pairs

    models/ â†’ saved FAISS index + reranker weights
    
    Deployment: CPU-only Docker image with mounted volumes for persistence

    
    ğŸ“‚ Project Layout
ai-kb-rag-pytorch/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI routes
â”‚   â”œâ”€â”€ rag_pipeline.py  # embed â†’ retrieve â†’ rerank â†’ generate
â”‚   â”œâ”€â”€ torch_reranker.py# PyTorch model + train/infer
â”‚   â”œâ”€â”€ generator.py     # flan-t5 inference (HF, CPU)
â”‚   â”œâ”€â”€ data_utils.py    # chunking, FAISS index, synthetic pairs
â”‚   â””â”€â”€ config.py
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ kb.txt           # Knowledge base (FAQs/policies)
â”‚   â””â”€â”€ pairs.jsonl      # (auto-generated) training pairs
â”‚â”€â”€ models/
â”‚   â””â”€â”€ reranker/        # fine-tuned PyTorch weights
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md


    âš¡ Quick Start

    1. Clone & build
git clone https://github.com/<your-username>/ai-kb-rag-pytorch.git
cd ai-kb-rag-pytorch

docker build -t ai-kb-rag:cpu .

    2. Run container
docker run --rm -p 8000:8000 \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/models:/app/models" \
  ai-kb-rag:cpu

    3. Check health
curl http://127.0.0.1:8000/health
# {"status":"ok"}

    ğŸ” Example Queries
# Ask a question
curl -s -X POST http://127.0.0.1:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question":"How to request a refund?"}' | jq

    Response:
{
  "answer": "Email support@acme.com with your order ID and reason. Refunds are processed in 5â€“7 business days.",
  "sources": [
    {"chunk_id": 0, "score": 1.0, "snippet": "Email support@acme.com with your order ID and reason..."}
  ]
}

    Train the reranker
curl -X POST http://127.0.0.1:8000/train
# {"status":"training_started"}
ğŸ› ï¸ Roadmap
 Redis cache for repeated queries
 More KB formats (PDF ingestion)
 TorchScript export for reranker (faster CPU inference)


